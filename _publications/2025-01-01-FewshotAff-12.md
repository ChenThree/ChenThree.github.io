---
title: "Variation-robust Few-shot 3D Affordance Segmentation for Robotic Manipulation"
collection: publications
permalink: /publication/2025-01-01-FewshotAff
excerpt: "Traditional affordance segmentation on 3D point cloud objects requires massive amounts of annotated training data and can only make predictions within predefined classes and affordance tasks. ..."
date: 2025-01-01
venue: 'IEEE Robotics and Automation Letters'
paperurl: 'https://ieeexplore.ieee.org/abstract/document/10819648/'
paperurltext: '[pdf]'
paperbibtex: 'https://scholar.googleusercontent.com/scholar.bib?q=info:ineQvD0GbWEJ:scholar.google.com/&output=citation&scisdr=ClE57TGEEJW3oGuqyLk:AFWwaeYAAAAAZ9Ks0Lm0i8AWM7u2mvL4zMvFOLc&scisig=AFWwaeYAAAAAZ9Ks0MW-FMEBW_vaePxJTPEBJlo&scisf=4&ct=citation&cd=-1&hl=en'
citation: 'Dingchang Hu, Tianyu Sun, Pengwei Xie, Siang Chen, Yixiang Dai, Huazhong Yang, Guijin Wang. (2024). Variation-robust Few-shot 3D Affordance Segmentation for Robotic Manipulation.'
---
## Abstract

Traditional affordance segmentation on 3D point cloud objects requires massive amounts of annotated training data and can only make predictions within predefined classes and affordance tasks. To overcome these limitations, we propose a variation-robust few-shot 3D affordance segmentation network (VRNet) for robotic manipulation, which requires only several affordance annotations for novel object classes and manipulation tasks. In particular, we design an orientation-tolerant feature extractor to address pose variation between support and query point cloud objects, and present a multi-scale label propagation algorithm for variation in completeness. Extensive experiments on affordance datasets show that VRNet provides the best segmentation performance compared with previous works. Moreover, experiments in real robotic scenarios demonstrate the generalization ability of our method.